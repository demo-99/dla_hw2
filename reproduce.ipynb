{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "reproduce.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWYxgVfkTuxI"
      },
      "source": [
        "Все логи обучения в code.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lhrn5O-qUYZ"
      },
      "source": [
        "# Import and misc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meO-Mp9jiAFC",
        "outputId": "a4df6077-684a-402a-adfb-b2181e89ccd3"
      },
      "source": [
        "!pip install torch==1.10.0\n",
        "!pip install torchaudio==0.10.0\n",
        "!pip install thop"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0) (3.10.0.2)\n",
            "Requirement already satisfied: torchaudio==0.10.0 in /usr/local/lib/python3.7/dist-packages (0.10.0+cu111)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio==0.10.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchaudio==0.10.0) (3.10.0.2)\n",
            "Collecting thop\n",
            "  Downloading thop-0.0.31.post2005241907-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from thop) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->thop) (3.10.0.2)\n",
            "Installing collected packages: thop\n",
            "Successfully installed thop-0.0.31.post2005241907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbUpoArCqUYa"
      },
      "source": [
        "from typing import Tuple, Union, List, Callable, Optional\n",
        "from tqdm import tqdm\n",
        "from itertools import islice\n",
        "import pathlib\n",
        "import dataclasses\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch import distributions\n",
        "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import torchaudio\n",
        "from IPython import display as display_\n",
        "\n",
        "\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "812GwLfqqUYf"
      },
      "source": [
        "# Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1DuQIyRqUYf"
      },
      "source": [
        "In this notebook we will implement a model for finding a keyword in a stream.\n",
        "\n",
        "We will implement the version with CRNN because it is easy and improves the model. \n",
        "(from https://www.dropbox.com/s/22ah2ba7dug6pzw/KWS_Attention.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PdhApeEh9pH"
      },
      "source": [
        "@dataclasses.dataclass\n",
        "class TaskConfig:\n",
        "    keyword: str = 'sheila'  # We will use 1 key word -- 'sheila'\n",
        "    batch_size: int = 256\n",
        "    learning_rate: float = 3e-4\n",
        "    weight_decay: float = 1e-5\n",
        "    num_epochs: int = 20\n",
        "    n_mels: int = 40\n",
        "    cnn_out_channels: int = 8\n",
        "    kernel_size: Tuple[int, int] = (5, 20)\n",
        "    stride: Tuple[int, int] = (2, 8)\n",
        "    hidden_size: int = 64\n",
        "    gru_num_layers: int = 2\n",
        "    bidirectional: bool = False\n",
        "    num_classes: int = 2\n",
        "    sample_rate: int = 16000\n",
        "    device: torch.device = torch.device(\n",
        "        'cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA1gPmE1h9pI"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2N8zcx9MF1X",
        "outputId": "cfad3dc1-2a2c-4cd4-dc63-0985ca2cadf3"
      },
      "source": [
        "!wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz -O speech_commands_v0.01.tar.gz\n",
        "!mkdir speech_commands && tar -C speech_commands -xvzf speech_commands_v0.01.tar.gz 1> log"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-22 15:38:41--  http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 172.217.218.128, 2a00:1450:4013:c08::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|172.217.218.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1489096277 (1.4G) [application/gzip]\n",
            "Saving to: ‘speech_commands_v0.01.tar.gz’\n",
            "\n",
            "speech_commands_v0. 100%[===================>]   1.39G   103MB/s    in 16s     \n",
            "\n",
            "2021-11-22 15:38:58 (88.5 MB/s) - ‘speech_commands_v0.01.tar.gz’ saved [1489096277/1489096277]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12wBTK0mNUsG"
      },
      "source": [
        "class SpeechCommandDataset(Dataset):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        transform: Optional[Callable] = None,\n",
        "        path2dir: str = None,\n",
        "        keywords: Union[str, List[str]] = None,\n",
        "        csv: Optional[pd.DataFrame] = None\n",
        "    ):        \n",
        "        self.transform = transform\n",
        "\n",
        "        if csv is None:\n",
        "            path2dir = pathlib.Path(path2dir)\n",
        "            keywords = keywords if isinstance(keywords, list) else [keywords]\n",
        "            \n",
        "            all_keywords = [\n",
        "                p.stem for p in path2dir.glob('*')\n",
        "                if p.is_dir() and not p.stem.startswith('_')\n",
        "            ]\n",
        "\n",
        "            triplets = []\n",
        "            for keyword in all_keywords:\n",
        "                paths = (path2dir / keyword).rglob('*.wav')\n",
        "                if keyword in keywords:\n",
        "                    for path2wav in paths:\n",
        "                        triplets.append((path2wav.as_posix(), keyword, 1))\n",
        "                else:\n",
        "                    for path2wav in paths:\n",
        "                        triplets.append((path2wav.as_posix(), keyword, 0))\n",
        "            \n",
        "            self.csv = pd.DataFrame(\n",
        "                triplets,\n",
        "                columns=['path', 'keyword', 'label']\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            self.csv = csv\n",
        "    \n",
        "    def __getitem__(self, index: int):\n",
        "        instance = self.csv.iloc[index]\n",
        "\n",
        "        path2wav = instance['path']\n",
        "        wav, sr = torchaudio.load(path2wav)\n",
        "        wav = wav.sum(dim=0)\n",
        "        \n",
        "        if self.transform:\n",
        "            wav = self.transform(wav)\n",
        "\n",
        "        return {\n",
        "            'wav': wav,\n",
        "            'keywors': instance['keyword'],\n",
        "            'label': instance['label']\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.csv)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1rVkT81Pk90"
      },
      "source": [
        "dataset = SpeechCommandDataset(\n",
        "    path2dir='speech_commands', keywords=TaskConfig.keyword\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "DFwhAXdfQLIA",
        "outputId": "57aefbf5-18d0-4bd9-b40a-d34d2beea21b"
      },
      "source": [
        "dataset.csv.sample(5)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>keyword</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>50042</th>\n",
              "      <td>speech_commands/one/bfaf2000_nohash_0.wav</td>\n",
              "      <td>one</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50175</th>\n",
              "      <td>speech_commands/one/569455ff_nohash_0.wav</td>\n",
              "      <td>one</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54676</th>\n",
              "      <td>speech_commands/go/6c968bd9_nohash_3.wav</td>\n",
              "      <td>go</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3579</th>\n",
              "      <td>speech_commands/bird/e82914c0_nohash_0.wav</td>\n",
              "      <td>bird</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54151</th>\n",
              "      <td>speech_commands/go/c2aeb59d_nohash_0.wav</td>\n",
              "      <td>go</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             path keyword  label\n",
              "50042   speech_commands/one/bfaf2000_nohash_0.wav     one      0\n",
              "50175   speech_commands/one/569455ff_nohash_0.wav     one      0\n",
              "54676    speech_commands/go/6c968bd9_nohash_3.wav      go      0\n",
              "3579   speech_commands/bird/e82914c0_nohash_0.wav    bird      0\n",
              "54151    speech_commands/go/c2aeb59d_nohash_0.wav      go      0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUxfDJw1qUYi"
      },
      "source": [
        "### Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmkxPWQqUYe"
      },
      "source": [
        "class AugsCreation:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.background_noises = [\n",
        "            'speech_commands/_background_noise_/white_noise.wav',\n",
        "            'speech_commands/_background_noise_/dude_miaowing.wav',\n",
        "            'speech_commands/_background_noise_/doing_the_dishes.wav',\n",
        "            'speech_commands/_background_noise_/exercise_bike.wav',\n",
        "            'speech_commands/_background_noise_/pink_noise.wav',\n",
        "            'speech_commands/_background_noise_/running_tap.wav'\n",
        "        ]\n",
        "\n",
        "        self.noises = [\n",
        "            torchaudio.load(p)[0].squeeze()\n",
        "            for p in self.background_noises\n",
        "        ]\n",
        "\n",
        "    def add_rand_noise(self, audio):\n",
        "\n",
        "        # randomly choose noise\n",
        "        noise_num = torch.randint(low=0, high=len(\n",
        "            self.background_noises), size=(1,)).item()\n",
        "        noise = self.noises[noise_num]\n",
        "\n",
        "        noise_level = torch.Tensor([1])  # [0, 40]\n",
        "\n",
        "        noise_energy = torch.norm(noise)\n",
        "        audio_energy = torch.norm(audio)\n",
        "        alpha = (audio_energy / noise_energy) * \\\n",
        "            torch.pow(10, -noise_level / 20)\n",
        "\n",
        "        start = torch.randint(\n",
        "            low=0,\n",
        "            high=max(int(noise.size(0) - audio.size(0) - 1), 1),\n",
        "            size=(1,)\n",
        "        ).item()\n",
        "        noise_sample = noise[start: start + audio.size(0)]\n",
        "\n",
        "        audio_new = audio + alpha * noise_sample\n",
        "        audio_new.clamp_(-1, 1)\n",
        "        return audio_new\n",
        "\n",
        "    def __call__(self, wav):\n",
        "        aug_num = torch.randint(low=0, high=4, size=(1,)).item()   # choose 1 random aug from augs\n",
        "        augs = [\n",
        "            lambda x: x,\n",
        "            lambda x: (x + distributions.Normal(0, 0.01).sample(x.size())).clamp_(-1, 1),\n",
        "            lambda x: torchaudio.transforms.Vol(.25)(x),\n",
        "            lambda x: self.add_rand_noise(x)\n",
        "        ]\n",
        "\n",
        "        return augs[aug_num](wav)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClWThxyYh9pM"
      },
      "source": [
        "indexes = torch.randperm(len(dataset))\n",
        "train_indexes = indexes[:int(len(dataset) * 0.8)]\n",
        "val_indexes = indexes[int(len(dataset) * 0.8):]\n",
        "\n",
        "train_df = dataset.csv.iloc[train_indexes].reset_index(drop=True)\n",
        "val_df = dataset.csv.iloc[val_indexes].reset_index(drop=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDPLht5fqUYe"
      },
      "source": [
        "# Sample is a dict of utt, word and label\n",
        "train_set = SpeechCommandDataset(csv=train_df, transform=AugsCreation())\n",
        "val_set = SpeechCommandDataset(csv=val_df)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmrJd8WIhkLP"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vbPDqd6qUYj"
      },
      "source": [
        "### Sampler for oversampling:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfnjRKo2qUYj"
      },
      "source": [
        "# We should provide to WeightedRandomSampler _weight for every sample_; by default it is 1/len(target)\n",
        "\n",
        "def get_sampler(target):\n",
        "    class_sample_count = np.array(\n",
        "        [len(np.where(target == t)[0]) for t in np.unique(target)])   # for every class count it's number of occ.\n",
        "    weight = 1. / class_sample_count\n",
        "    samples_weight = np.array([weight[t] for t in target])\n",
        "    samples_weight = torch.from_numpy(samples_weight)\n",
        "    samples_weigth = samples_weight.float()\n",
        "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
        "    return sampler"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM8gLmHeqUYj"
      },
      "source": [
        "train_sampler = get_sampler(train_set.csv['label'].values)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyBqbxp0h9pO"
      },
      "source": [
        "class Collator:\n",
        "    \n",
        "    def __call__(self, data):\n",
        "        wavs = []\n",
        "        labels = []    \n",
        "\n",
        "        for el in data:\n",
        "            wavs.append(el['wav'])\n",
        "            labels.append(el['label'])\n",
        "\n",
        "        # torch.nn.utils.rnn.pad_sequence takes list(Tensors) and returns padded (with 0.0) Tensor\n",
        "        wavs = pad_sequence(wavs, batch_first=True)    \n",
        "        labels = torch.Tensor(labels).long()\n",
        "        return wavs, labels"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8G9xPRVqUYk"
      },
      "source": [
        "###  Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wGBMcQiqUYk"
      },
      "source": [
        "# Here we are obliged to use shuffle=False because of our sampler with randomness inside.\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=TaskConfig.batch_size,\n",
        "                          shuffle=False, collate_fn=Collator(),\n",
        "                          sampler=train_sampler,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "val_loader = DataLoader(val_set, batch_size=TaskConfig.batch_size,\n",
        "                        shuffle=False, collate_fn=Collator(),\n",
        "                        num_workers=2, pin_memory=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTlsn6cpqUYk"
      },
      "source": [
        "### Creating MelSpecs on GPU for speeeed: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRXMt6it56fW"
      },
      "source": [
        "class LogMelspec:\n",
        "\n",
        "    def __init__(self, is_train, config):\n",
        "        # with augmentations\n",
        "        if is_train:\n",
        "            self.melspec = nn.Sequential(\n",
        "                torchaudio.transforms.MelSpectrogram(\n",
        "                    sample_rate=config.sample_rate,\n",
        "                    n_fft=400,\n",
        "                    win_length=400,\n",
        "                    hop_length=160,\n",
        "                    n_mels=config.n_mels\n",
        "                ),\n",
        "                torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
        "                torchaudio.transforms.TimeMasking(time_mask_param=35),\n",
        "            ).to(config.device)\n",
        "\n",
        "        # no augmentations\n",
        "        else:\n",
        "            self.melspec = torchaudio.transforms.MelSpectrogram(\n",
        "                sample_rate=config.sample_rate,\n",
        "                n_fft=400,\n",
        "                win_length=400,\n",
        "                hop_length=160,\n",
        "                n_mels=config.n_mels\n",
        "            ).to(config.device)\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        # already on device\n",
        "        return torch.log(self.melspec(batch).clamp_(min=1e-9, max=1e9))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqkz4_gn8BiF"
      },
      "source": [
        "melspec_train = LogMelspec(is_train=True, config=TaskConfig)\n",
        "melspec_val = LogMelspec(is_train=False, config=TaskConfig)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoAxmihY8yxr"
      },
      "source": [
        "### Quality measurment functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euwD1UyuqUYk"
      },
      "source": [
        "# FA - true: 0, model: 1\n",
        "# FR - true: 1, model: 0\n",
        "\n",
        "def count_FA_FR(preds, labels):\n",
        "    FA = torch.sum(preds[labels == 0])\n",
        "    FR = torch.sum(labels[preds == 0])\n",
        "    \n",
        "    # torch.numel - returns total number of elements in tensor\n",
        "    return FA.item() / torch.numel(preds), FR.item() / torch.numel(preds)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHBUrkT1qUYk"
      },
      "source": [
        "def get_au_fa_fr(probs, labels):\n",
        "    sorted_probs, _ = torch.sort(probs)\n",
        "    sorted_probs = torch.cat((torch.Tensor([0]), sorted_probs, torch.Tensor([1])))\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "        \n",
        "    FAs, FRs = [], []\n",
        "    for prob in sorted_probs:\n",
        "        preds = (probs >= prob) * 1\n",
        "        FA, FR = count_FA_FR(preds, labels)        \n",
        "        FAs.append(FA)\n",
        "        FRs.append(FR)\n",
        "    # plt.plot(FAs, FRs)\n",
        "    # plt.show()\n",
        "\n",
        "    # ~ area under curve using trapezoidal rule\n",
        "    return -np.trapz(FRs, x=FAs)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcEP5cEZqUYl"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cP_pFIsy5p2",
        "outputId": "ace65869-4b35-4b07-d2d7-2722afa85577"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.energy = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, input):\n",
        "        energy = self.energy(input)\n",
        "        alpha = torch.softmax(energy, dim=-2)\n",
        "        return (input * alpha).sum(dim=-2)\n",
        "\n",
        "class CRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, config: TaskConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1, out_channels=config.cnn_out_channels,\n",
        "                kernel_size=config.kernel_size, stride=config.stride\n",
        "            ),\n",
        "            nn.Flatten(start_dim=1, end_dim=2),\n",
        "        )\n",
        "\n",
        "        self.conv_out_frequency = (config.n_mels - config.kernel_size[0]) // \\\n",
        "            config.stride[0] + 1\n",
        "        \n",
        "        self.gru = nn.GRU(\n",
        "            input_size=self.conv_out_frequency * config.cnn_out_channels,\n",
        "            hidden_size=config.hidden_size,\n",
        "            num_layers=config.gru_num_layers,\n",
        "            dropout=0.1,\n",
        "            bidirectional=config.bidirectional,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.attention = Attention(config.hidden_size)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        input = input.unsqueeze(dim=1)\n",
        "        conv_output = self.conv(input).transpose(-1, -2)\n",
        "        gru_output, _ = self.gru(conv_output)\n",
        "        contex_vector = self.attention(gru_output)\n",
        "        output = self.classifier(contex_vector)\n",
        "        return output\n",
        "\n",
        "config = TaskConfig()\n",
        "model = CRNN(config)\n",
        "model"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRNN(\n",
              "  (conv): Sequential(\n",
              "    (0): Conv2d(1, 8, kernel_size=(5, 20), stride=(2, 8))\n",
              "    (1): Flatten(start_dim=1, end_dim=2)\n",
              "  )\n",
              "  (gru): GRU(144, 64, num_layers=2, batch_first=True, dropout=0.1)\n",
              "  (attention): Attention(\n",
              "    (energy): Sequential(\n",
              "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
              "      (1): Tanh()\n",
              "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=64, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmmSFvWaqUYn"
      },
      "source": [
        "def train_epoch(model, opt, loader, log_melspec, device):\n",
        "    model.train()\n",
        "    for i, (batch, labels) in tqdm(enumerate(loader), total=len(loader)):\n",
        "        batch, labels = batch.to(device), labels.to(device)\n",
        "        batch = log_melspec(batch)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # run model # with autocast():\n",
        "        logits = model(batch)\n",
        "        # we need probabilities so we use softmax & CE separately\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "\n",
        "        opt.step()\n",
        "\n",
        "        # logging\n",
        "        argmax_probs = torch.argmax(probs, dim=-1)\n",
        "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
        "        acc = torch.sum(argmax_probs == labels) / torch.numel(argmax_probs)\n",
        "\n",
        "    return acc"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIeRbn4tqUYo"
      },
      "source": [
        "@torch.no_grad()\n",
        "def validation(model, loader, log_melspec, device):\n",
        "    model.eval()\n",
        "\n",
        "    val_losses, accs, FAs, FRs = [], [], [], []\n",
        "    all_probs, all_labels = [], []\n",
        "    for i, (batch, labels) in tqdm(enumerate(loader)):\n",
        "        batch, labels = batch.to(device), labels.to(device)\n",
        "        batch = log_melspec(batch)\n",
        "\n",
        "        output = model(batch)\n",
        "        # we need probabilities so we use softmax & CE separately\n",
        "        probs = F.softmax(output, dim=-1)\n",
        "        loss = F.cross_entropy(output, labels)\n",
        "\n",
        "        # logging\n",
        "        argmax_probs = torch.argmax(probs, dim=-1)\n",
        "        all_probs.append(probs[:, 1].cpu())\n",
        "        all_labels.append(labels.cpu())\n",
        "        val_losses.append(loss.item())\n",
        "        accs.append(\n",
        "            torch.sum(argmax_probs == labels).item() /  # ???\n",
        "            torch.numel(argmax_probs)\n",
        "        )\n",
        "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
        "        FAs.append(FA)\n",
        "        FRs.append(FR)\n",
        "\n",
        "    # area under FA/FR curve for whole loader\n",
        "    au_fa_fr = get_au_fa_fr(torch.cat(all_probs, dim=0).cpu(), all_labels)\n",
        "    return au_fa_fr"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpyvKwp0k3IU"
      },
      "source": [
        "from collections import defaultdict\n",
        "from IPython.display import clear_output\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "history = defaultdict(list)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwSPVnMI8-iK",
        "outputId": "db376493-8636-45c8-f69f-98540a775afa"
      },
      "source": [
        "from thop import profile  # !pip install thop\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Conv1d(1, 1, 3, bias=False)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "        \n",
        "profile(CRNN(config).to(config.device), (torch.randn(128, 40, 101).to(config.device), ))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.flatten.Flatten'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "[INFO] Register count_gru() for <class 'torch.nn.modules.rnn.GRU'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Tanh'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "\u001b[91m[WARN] Cannot find rule for <class '__main__.Attention'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "\u001b[91m[WARN] Cannot find rule for <class '__main__.CRNN'>. Treat it as zero Macs and zero Params.\u001b[00m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(119527424.0, 70443.0)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHKgLU5_9D9V"
      },
      "source": [
        "import tempfile\n",
        "\n",
        "def get_size_in_kilobytes(model, label=\"\"):\n",
        "    # https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html#look-at-model-size\n",
        "    with tempfile.TemporaryFile() as f:\n",
        "        torch.save(model.state_dict(), f)\n",
        "        size = f.tell() / 2**10\n",
        "        print(\"model: \",label,' \\t','Size (KB):', size)\n",
        "    return size"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N2EuXRaakCu"
      },
      "source": [
        "# Dark Knowledge Distilation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcoeZ9eHso-P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83744325-1073-4954-cd97-310cf3d021e8"
      },
      "source": [
        "!gdown --id 1TMDjzv-73MDL2lt6Lypv2w-mjOScroFK"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1TMDjzv-73MDL2lt6Lypv2w-mjOScroFK\n",
            "To: /content/checkpoint_dla_basemodel\n",
            "\r  0% 0.00/285k [00:00<?, ?B/s]\r100% 285k/285k [00:00<00:00, 46.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksoAXfpmebh2"
      },
      "source": [
        "model.load_state_dict(torch.load('checkpoint_dla_basemodel'))\n",
        "model = model.to(config.device)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93mu0eg1cIcQ"
      },
      "source": [
        "def train_epoch_distillated(model, opt, loader, log_melspec, device, trained_model, alpha=0.5, T=4):\n",
        "    model.train()\n",
        "    trained_model.eval()\n",
        "    for i, (batch, labels) in tqdm(enumerate(loader), total=len(loader)):\n",
        "        batch, labels = batch.to(device), labels.to(device)\n",
        "        batch = log_melspec(batch)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        logits = model(batch)\n",
        "        with torch.no_grad():\n",
        "            teacher_preds = trained_model(batch) / T\n",
        "            teacher_probs = F.softmax(teacher_preds, dim=-1)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        loss = T**2 * alpha * F.cross_entropy(logits / T, teacher_probs)\n",
        "        loss += (1 - alpha) * F.cross_entropy(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "\n",
        "        opt.step()\n",
        "\n",
        "        # logging\n",
        "        argmax_probs = torch.argmax(probs, dim=-1)\n",
        "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
        "        acc = torch.sum(argmax_probs == labels) / torch.numel(argmax_probs)\n",
        "\n",
        "    return acc"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjRNOxg4dpFT"
      },
      "source": [
        "history = defaultdict(list)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NE0MKWtvkOWQ",
        "outputId": "4ec72954-f3cd-4388-a722-5c1abdf8930c"
      },
      "source": [
        "config = TaskConfig(cnn_out_channels=3, hidden_size=16, gru_num_layers=2, kernel_size=(3, 10), num_epochs=110)\n",
        "dark_knowledge_model = CRNN(config).to(config.device)\n",
        "\n",
        "print(dark_knowledge_model)\n",
        "\n",
        "opt = torch.optim.Adam(\n",
        "    dark_knowledge_model.parameters(),\n",
        "    lr=config.learning_rate,\n",
        "    weight_decay=config.weight_decay\n",
        ")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CRNN(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(1, 3, kernel_size=(3, 10), stride=(2, 8))\n",
            "    (1): Flatten(start_dim=1, end_dim=2)\n",
            "  )\n",
            "  (gru): GRU(57, 16, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (attention): Attention(\n",
            "    (energy): Sequential(\n",
            "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
            "      (1): Tanh()\n",
            "      (2): Linear(in_features=16, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Linear(in_features=16, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cR477TdEkwUm",
        "outputId": "0de2cd72-e387-4851-80c6-ea814c057311"
      },
      "source": [
        "profile(CRNN(TaskConfig()).to(config.device), (torch.randn(128, 40, 101).to(config.device), ))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.flatten.Flatten'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "[INFO] Register count_gru() for <class 'torch.nn.modules.rnn.GRU'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Tanh'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "\u001b[91m[WARN] Cannot find rule for <class '__main__.Attention'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "\u001b[91m[WARN] Cannot find rule for <class '__main__.CRNN'>. Treat it as zero Macs and zero Params.\u001b[00m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(119527424.0, 70443.0)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5l8BleskwUn",
        "outputId": "61759ad0-cd57-4471-a334-544dee9fddbe"
      },
      "source": [
        "profile(CRNN(config).to(config.device), (torch.randn(128, 40, 101).to(config.device), ))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.flatten.Flatten'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "[INFO] Register count_gru() for <class 'torch.nn.modules.rnn.GRU'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Tanh'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "\u001b[91m[WARN] Cannot find rule for <class '__main__.Attention'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "\u001b[91m[WARN] Cannot find rule for <class '__main__.CRNN'>. Treat it as zero Macs and zero Params.\u001b[00m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11516416.0, 5648.0)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWM5RTQBRMEj",
        "outputId": "e77773a7-8053-4d21-941a-96515a3bf32c"
      },
      "source": [
        "!gdown --id 1pn9bU6AtY-oFDRgOO7l5NfuuVZaA_NfQ"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1pn9bU6AtY-oFDRgOO7l5NfuuVZaA_NfQ\n",
            "To: /content/checkpoint_dla_distillated\n",
            "\r  0% 0.00/26.1k [00:00<?, ?B/s]\r100% 26.1k/26.1k [00:00<00:00, 21.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLxqkiGPRMEk"
      },
      "source": [
        "dark_knowledge_model.load_state_dict(torch.load('checkpoint_dla_distillated'))\n",
        "dark_knowledge_model = dark_knowledge_model.to(config.device)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K8IIGevSSg5"
      },
      "source": [
        "# Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPtn-5tZjbC5"
      },
      "source": [
        "### Creating MelSpecs on CPU for not using nightly releasy with ability to quantize on CUDA: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnc9T-gIgvdB"
      },
      "source": [
        "config = TaskConfig(device=torch.device('cpu'))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97IslIX2jbDB"
      },
      "source": [
        "melspec_train = LogMelspec(is_train=True, config=config)\n",
        "melspec_val = LogMelspec(is_train=False, config=config)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBWhBA3Zkgv7"
      },
      "source": [
        "### Dynamic quantization int8 for GRU + Linear (coz Convs are not supported for dynamic)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-HLoM1YSRhE"
      },
      "source": [
        "model_int8 = torch.quantization.quantize_dynamic(\n",
        "    model.to('cpu'),  # the original model\n",
        "    {nn.GRU, nn.Linear},  # a set of layers to dynamically quantize\n",
        "    dtype=torch.qint8)  # the target dtype for quantized weights"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdwaYeWEjSZE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "783f9f95-9c1c-49bb-a3b8-1be410a59936"
      },
      "source": [
        "validation(model_int8, val_loader,\n",
        "           melspec_val, config.device)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "51it [00:10,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.682251663118214e-05"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOVV03nAdhOh",
        "outputId": "e68feba3-e975-478f-e096-60ad567dcf46"
      },
      "source": [
        "print('The size of the model in KB has decreased by {} times'.format(\n",
        "    get_size_in_kilobytes(model, label='base') / get_size_in_kilobytes(model_int8, label='dynamically quantized')))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model:  base  \t Size (KB): 279.5517578125\n",
            "model:  dynamically quantized  \t Size (KB): 80.7841796875\n",
            "The size of the model in KB has decreased by 3.4604765301065 times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaC7AF3Gm1Sy"
      },
      "source": [
        "### Dynamic quantization float16 for GRU + Linear (coz Convs are not supported for dynamic)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hp6DQiom1Sz"
      },
      "source": [
        "model_float16 = torch.quantization.quantize_dynamic(\n",
        "    model.to('cpu'),  # the original model\n",
        "    {nn.GRU, nn.Linear},  # a set of layers to dynamically quantize\n",
        "    dtype=torch.float16)  # the target dtype for quantized weights"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lz65JG7Lm1Sz",
        "outputId": "b72ebe36-f2de-4e8f-c257-dba26dc038cb"
      },
      "source": [
        "validation(model_float16, val_loader,\n",
        "           melspec_val, config.device)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "51it [00:09,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.5891579208527146e-05"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wr4cElr_m1Sz",
        "outputId": "2f37142e-56ad-4e9f-a618-02671fb0320c"
      },
      "source": [
        "print('The size of the model in KB has decreased by {} times'.format(\n",
        "    get_size_in_kilobytes(model, label='base') / get_size_in_kilobytes(model_float16, label='dynamically quantized')))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model:  base  \t Size (KB): 279.5517578125\n",
            "model:  dynamically quantized  \t Size (KB): 282.1591796875\n",
            "The size of the model in KB has decreased by 0.9907590393554171 times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jfl_mOEQCsdC"
      },
      "source": [
        "# Dynamic quantization int8 for GRU + Linear (coz Convs are not supported for dynamic) after distillation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b9vocjHCsdC"
      },
      "source": [
        "config = TaskConfig(device=torch.device('cpu'))"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5umoijeUCsdD"
      },
      "source": [
        "distillated_model_int8 = torch.quantization.quantize_dynamic(\n",
        "    dark_knowledge_model.to('cpu'),  # the original model\n",
        "    {nn.GRU, nn.Linear},  # a set of layers to dynamically quantize\n",
        "    dtype=torch.qint8)  # the target dtype for quantized weights"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsCViIJjCsdD",
        "outputId": "3b42c0ad-3574-4de9-8867-97db57b327d6"
      },
      "source": [
        "validation(distillated_model_int8, val_loader,\n",
        "           melspec_val, config.device)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "51it [00:09,  5.64it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.215314636708384e-05"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYN5x8ayETQF"
      },
      "source": [
        "### Comparison with distillated model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r81LkqahCsdD",
        "outputId": "adf992e2-3334-4ea7-9dd8-f0606f9b453c"
      },
      "source": [
        "print('The size of the model in KB has decreased by {} times compared to distillated model'.format(\n",
        "    get_size_in_kilobytes(dark_knowledge_model, label='distillated') / get_size_in_kilobytes(distillated_model_int8, label='distillated and dynamically quantized')))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model:  distillated  \t Size (KB): 26.4892578125\n",
            "model:  distillated and dynamically quantized  \t Size (KB): 13.5966796875\n",
            "The size of the model in KB has decreased by 1.9482151835093011 times compared to distillated model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpab0wDhEZdh"
      },
      "source": [
        "### Comparison with base model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCwpV-SDESBx",
        "outputId": "3664eefe-8210-4066-b688-6c73e6a79094"
      },
      "source": [
        "print('The size of the model in KB has decreased by {} times compared to base model'.format(\n",
        "    get_size_in_kilobytes(model, label='base') / get_size_in_kilobytes(distillated_model_int8, label='distillated and dynamically quantized')))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model:  base  \t Size (KB): 279.5517578125\n",
            "model:  distillated and dynamically quantized  \t Size (KB): 13.5966796875\n",
            "The size of the model in KB has decreased by 20.56029591323709 times compared to base model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2PtlePXoMdq"
      },
      "source": [
        "# Dynamic quantization float16 for GRU + Linear (coz Convs are not supported for dynamic) after distillation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4LSUoI7pRMZ"
      },
      "source": [
        "I have no clue what's going on here. Honestly, it should takes less memory but seems that some extra quantizatation tools inside model are taking too much memory or extra information about layers. Or maybe bug..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbA1ErrtoMdr"
      },
      "source": [
        "config = TaskConfig(device=torch.device('cpu'))"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1iUrFgMoMdr"
      },
      "source": [
        "distillated_model_float16 = torch.quantization.quantize_dynamic(\n",
        "    dark_knowledge_model.to('cpu'),  # the original model\n",
        "    {nn.GRU, nn.Linear},  # a set of layers to dynamically quantize\n",
        "    dtype=torch.float16)  # the target dtype for quantized weights"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DTh3go2oMdr",
        "outputId": "81db43f9-49d9-4076-e218-7678a0c7a118"
      },
      "source": [
        "validation(distillated_model_float16, val_loader,\n",
        "           melspec_val, config.device)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "51it [00:09,  5.58it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.097157194602174e-05"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHbzDamzptYS",
        "outputId": "fdaa60e0-0e38-4e18-f8e6-5ca343dcbb84"
      },
      "source": [
        "dark_knowledge_model"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRNN(\n",
              "  (conv): Sequential(\n",
              "    (0): Conv2d(1, 3, kernel_size=(3, 10), stride=(2, 8))\n",
              "    (1): Flatten(start_dim=1, end_dim=2)\n",
              "  )\n",
              "  (gru): GRU(57, 16, num_layers=2, batch_first=True, dropout=0.1)\n",
              "  (attention): Attention(\n",
              "    (energy): Sequential(\n",
              "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
              "      (1): Tanh()\n",
              "      (2): Linear(in_features=16, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=16, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ns_7AJpNpqIo",
        "outputId": "ef75b098-d45a-4b42-e9aa-57c6a4221b7e"
      },
      "source": [
        "distillated_model_float16"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRNN(\n",
              "  (conv): Sequential(\n",
              "    (0): Conv2d(1, 3, kernel_size=(3, 10), stride=(2, 8))\n",
              "    (1): Flatten(start_dim=1, end_dim=2)\n",
              "  )\n",
              "  (gru): DynamicQuantizedGRU(57, 16, num_layers=2, batch_first=True, dropout=0.1)\n",
              "  (attention): Attention(\n",
              "    (energy): Sequential(\n",
              "      (0): DynamicQuantizedLinear(in_features=16, out_features=16, dtype=torch.float16)\n",
              "      (1): Tanh()\n",
              "      (2): DynamicQuantizedLinear(in_features=16, out_features=1, dtype=torch.float16)\n",
              "    )\n",
              "  )\n",
              "  (classifier): DynamicQuantizedLinear(in_features=16, out_features=2, dtype=torch.float16)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7KWAebToMds"
      },
      "source": [
        "### Comparison with distillated model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sY9SpJwvoMds",
        "outputId": "ad6aa587-d27f-4cd1-c7f1-fdbfce6aa100"
      },
      "source": [
        "print('The size of the model in KB has decreased by {} times compared to distillated model'.format(\n",
        "    get_size_in_kilobytes(dark_knowledge_model, label='distillated') / get_size_in_kilobytes(distillated_model_float16, label='distillated and dynamically quantized')))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model:  distillated  \t Size (KB): 26.4892578125\n",
            "model:  distillated and dynamically quantized  \t Size (KB): 29.0966796875\n",
            "The size of the model in KB has decreased by 0.910387648934385 times compared to distillated model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H0D3kqT2rqN"
      },
      "source": [
        "# Streaming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQMevExUrnr2"
      },
      "source": [
        "DEFAULT_MAX_WINDOW_LENGTH = 50\n",
        "DEFAULT_STREAMING_STEP_SIZE = 20\n",
        "\n",
        "class CRNNWithStreaming(CRNN):\n",
        "\n",
        "    def __init__(self, config: TaskConfig):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.max_window_length = DEFAULT_MAX_WINDOW_LENGTH\n",
        "        if hasattr(config, 'max_window_length'):\n",
        "            self.max_window_length = max_window_length\n",
        "\n",
        "        self.streaming_step_size = DEFAULT_STREAMING_STEP_SIZE\n",
        "        if hasattr(config, 'streaming_step_size'):\n",
        "            self.streaming_step_size = streaming_step_size\n",
        "\n",
        "        self.gru_buffer = None\n",
        "    \n",
        "    def forward(self, input):\n",
        "        if self.training:\n",
        "            return super().forward(input)\n",
        "        \n",
        "        frame_pointer = self.max_window_length\n",
        "        hidden = None\n",
        "        outputs = None\n",
        "        while frame_pointer < input.size(-1):\n",
        "            frame = input[:, :, frame_pointer-self.max_window_length:frame_pointer].unsqueeze(dim=1)\n",
        "            conv_output = self.conv(frame).transpose(-1, -2)\n",
        "            gru_output, hidden = self.gru(conv_output, hidden)\n",
        "            if self.gru_buffer is None:\n",
        "                self.gru_buffer = gru_output\n",
        "            else:\n",
        "                self.gru_buffer = torch.cat((self.gru_buffer[:,-1:,:], gru_output), dim=1)\n",
        "            contex_vector = self.attention(gru_output)\n",
        "            output = self.classifier(contex_vector)\n",
        "            if outputs is None:\n",
        "                outputs = output.unsqueeze(0)\n",
        "            else:\n",
        "                outputs = torch.cat((outputs, output.unsqueeze(0)), dim=0)\n",
        "            frame_pointer += self.streaming_step_size\n",
        "            frame_pointer = min(frame_pointer, input.size(-1))\n",
        "        return outputs\n",
        "\n",
        "\n",
        "config = TaskConfig()\n",
        "\n",
        "melspec_train = LogMelspec(is_train=True, config=config)\n",
        "melspec_val = LogMelspec(is_train=False, config=config)\n",
        "\n",
        "model_with_streaming = CRNNWithStreaming(config)\n",
        "model_with_streaming.load_state_dict(torch.load('checkpoint_dla_basemodel'))\n",
        "model_with_streaming = model_with_streaming.to(config.device)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "gJVOGax3a92J",
        "outputId": "6a51a34c-e218-4485-cdde-4ddc0324ea93"
      },
      "source": [
        "@torch.no_grad()\n",
        "def streaming_plot(model, loader, log_melspec, device):\n",
        "    model.eval()\n",
        "\n",
        "    val_csv = val_set.csv\n",
        "    not_kw_obj = val_csv[val_csv['label'] == 0]['path'].values\n",
        "    not_kw = []\n",
        "    for i in not_kw_obj:\n",
        "        not_kw.append(str(i))\n",
        "        if len(not_kw) == 3:\n",
        "            break\n",
        "    kw = str(val_csv[val_csv['label'] != 0]['path'].values[0])\n",
        "\n",
        "    x = torch.cat([torchaudio.load(path)[0].squeeze() for path in (*not_kw, kw, *not_kw)]).to(device)\n",
        "    x = log_melspec(x).unsqueeze(0)\n",
        "\n",
        "    outputs = model(x)\n",
        "    probs = F.softmax(outputs, dim=-1)\n",
        "\n",
        "    plt.plot(list(range(probs.size(0))), probs[:, 0, 1].cpu())\n",
        "    plt.xlabel('window number')\n",
        "    plt.ylabel('keyword prob')\n",
        "    plt.show()\n",
        "\n",
        "streaming_plot(model_with_streaming, val_loader,\n",
        "           melspec_val, config.device)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyoJGhmIDFxX"
      },
      "source": [
        ""
      ],
      "execution_count": 56,
      "outputs": []
    }
  ]
}